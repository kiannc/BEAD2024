{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1f40daa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f40daa9",
        "outputId": "d54682e9-7101-4408-8400-6b42e8d39085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install pyspark and findspark\n",
        "!pip install --ignore-install -q pyspark\n",
        "# Install findspark library\n",
        "!pip install --ignore-install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "EPtkOYy2sXvV"
      },
      "id": "EPtkOYy2sXvV",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh6CVHoZsn7X",
        "outputId": "7f2e1a8d-1e52-4ae5-b1b5-6eea81e4bf6a"
      },
      "id": "Xh6CVHoZsn7X",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.version_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-rlBNeOujtB",
        "outputId": "727bf726-2d84-48cd-dc62-89b68c0b635d"
      },
      "id": "g-rlBNeOujtB",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08752f4a",
      "metadata": {
        "id": "08752f4a"
      },
      "source": [
        "### 1 Import libraries and create spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3d447a24",
      "metadata": {
        "id": "3d447a24"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType\n",
        "from sklearn.datasets import load_iris\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"MLPipeline_IrisData\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f22f66e",
      "metadata": {
        "id": "3f22f66e"
      },
      "source": [
        "### 2. Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "faa1ebb1",
      "metadata": {
        "id": "faa1ebb1"
      },
      "outputs": [],
      "source": [
        "# Load Iris dataset using scikit-learn\n",
        "iris = load_iris()\n",
        "iris_data = iris.data\n",
        "iris_target = iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba025350",
      "metadata": {
        "id": "ba025350"
      },
      "source": [
        "### 3. Define schema and convert into dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "872a672e",
      "metadata": {
        "id": "872a672e"
      },
      "outputs": [],
      "source": [
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"sepal_length\", DoubleType(), True),\n",
        "    StructField(\"sepal_width\", DoubleType(), True),\n",
        "    StructField(\"petal_length\", DoubleType(), True),\n",
        "    StructField(\"petal_width\", DoubleType(), True),\n",
        "    StructField(\"label\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Convert the data to a DataFrame with the specified schema\n",
        "data = spark.createDataFrame(\n",
        "    [(float(x[0]), float(x[1]), float(x[2]), float(x[3]), int(y)) for x, y in zip(iris_data, iris_target)],\n",
        "    schema=schema\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "300d9f62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "300d9f62",
        "outputId": "d05021a0-85c7-4bd2-bcb6-82672c831d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|label|\n",
            "+------------+-----------+------------+-----------+-----+\n",
            "|         5.1|        3.5|         1.4|        0.2|    0|\n",
            "|         4.9|        3.0|         1.4|        0.2|    0|\n",
            "|         4.7|        3.2|         1.3|        0.2|    0|\n",
            "|         4.6|        3.1|         1.5|        0.2|    0|\n",
            "|         5.0|        3.6|         1.4|        0.2|    0|\n",
            "|         5.4|        3.9|         1.7|        0.4|    0|\n",
            "|         4.6|        3.4|         1.4|        0.3|    0|\n",
            "|         5.0|        3.4|         1.5|        0.2|    0|\n",
            "|         4.4|        2.9|         1.4|        0.2|    0|\n",
            "|         4.9|        3.1|         1.5|        0.1|    0|\n",
            "|         5.4|        3.7|         1.5|        0.2|    0|\n",
            "|         4.8|        3.4|         1.6|        0.2|    0|\n",
            "|         4.8|        3.0|         1.4|        0.1|    0|\n",
            "|         4.3|        3.0|         1.1|        0.1|    0|\n",
            "|         5.8|        4.0|         1.2|        0.2|    0|\n",
            "|         5.7|        4.4|         1.5|        0.4|    0|\n",
            "|         5.4|        3.9|         1.3|        0.4|    0|\n",
            "|         5.1|        3.5|         1.4|        0.3|    0|\n",
            "|         5.7|        3.8|         1.7|        0.3|    0|\n",
            "|         5.1|        3.8|         1.5|        0.3|    0|\n",
            "+------------+-----------+------------+-----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bcaff7b",
      "metadata": {
        "id": "0bcaff7b"
      },
      "source": [
        "### 4. Split into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "1a9cff08",
      "metadata": {
        "id": "1a9cff08"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "(trainingData, testData) = data.randomSplit([0.8, 0.2], seed=1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e865bf23",
      "metadata": {
        "id": "e865bf23"
      },
      "source": [
        "### 5. Form feature columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "90f594c8",
      "metadata": {
        "id": "90f594c8"
      },
      "outputs": [],
      "source": [
        "# Define the feature columns\n",
        "feature_columns = data.columns\n",
        "feature_columns.remove(\"label\")\n",
        "\n",
        "# Create a vector assembler to assemble feature columns into a single feature vector\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Create a StringIndexer to convert labels to indices\n",
        "indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc677b5",
      "metadata": {
        "id": "6cc677b5"
      },
      "source": [
        "### 6. Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ba440c11",
      "metadata": {
        "id": "ba440c11"
      },
      "outputs": [],
      "source": [
        "# Create a RandomForestClassifier\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e8d9f5c",
      "metadata": {
        "id": "9e8d9f5c"
      },
      "source": [
        "### 7. Create Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2abe141e",
      "metadata": {
        "id": "2abe141e"
      },
      "outputs": [],
      "source": [
        "# Create a pipeline with the stages: vector assembler, label indexer, and random forest\n",
        "pipeline = Pipeline(stages=[assembler, indexer, rf])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9416ca78",
      "metadata": {
        "id": "9416ca78"
      },
      "source": [
        "### 8. Parameter searching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "92dbf24a",
      "metadata": {
        "id": "92dbf24a"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid for hyperparameter tuning\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(rf.maxDepth, [5, 10, 15])\n",
        "             .addGrid(rf.numTrees, [20, 50, 100])\n",
        "             .build())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aaee427",
      "metadata": {
        "id": "6aaee427"
      },
      "source": [
        "### 9. Cross evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "53751466",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53751466",
        "outputId": "c3962c50-1bc1-43df-f51e-9b6d029541d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.972973\n"
          ]
        }
      ],
      "source": [
        "# Create a multi-class classification evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Create a cross-validator with the pipeline, parameter grid, and evaluator\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=3)\n",
        "\n",
        "# Fit the cross-validator to the training data\n",
        "cvModel = crossval.fit(trainingData)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = cvModel.transform(testData)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy = %g\" % accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "73b3f047",
      "metadata": {
        "id": "73b3f047"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}