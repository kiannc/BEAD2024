{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "174cffa7",
      "metadata": {
        "id": "174cffa7"
      },
      "source": [
        "### PySpark-LogisticRegression\n",
        "1. use bank.csv data to do binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "49cdb04e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49cdb04e",
        "outputId": "721b236e-8437-46ff-93dd-b20e79348260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m604.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install pyspark and findspark\n",
        "!pip install --ignore-install -q pyspark\n",
        "# Install findspark library\n",
        "!pip install --ignore-install -q findspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "pAj7v8SdYE14"
      },
      "id": "pAj7v8SdYE14",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2e9aa612",
      "metadata": {
        "id": "2e9aa612"
      },
      "source": [
        "### 1.  Set up SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5f49b659",
      "metadata": {
        "id": "5f49b659"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "            .builder \\\n",
        "            .appName(\"Python Spark Logistic Regression example\") \\\n",
        "            .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQF1Lie3YSIJ",
        "outputId": "2e1f734e-e586-448f-9756-a53b49a17bd5"
      },
      "id": "BQF1Lie3YSIJ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd6e1fe1",
      "metadata": {
        "id": "dd6e1fe1"
      },
      "source": [
        "### 2. Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0d048dce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d048dce",
        "outputId": "891355bf-d397-45cc-bf29-7040905ca0be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
            "|age|        job|marital|education|default|balance|housing|loan| contact|duration|campaign|pdays|previous|  y|\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
            "| 30| unemployed|married|  primary|     no|   1787|     no|  no|cellular|      79|       1|   -1|       0|  0|\n",
            "| 33|   services|married|secondary|     no|   4789|    yes| yes|cellular|     220|       1|  339|       4|  0|\n",
            "| 35| management| single| tertiary|     no|   1350|    yes|  no|cellular|     185|       1|  330|       1|  0|\n",
            "| 30| management|married| tertiary|     no|   1476|    yes| yes| unknown|     199|       4|   -1|       0|  0|\n",
            "| 59|blue-collar|married|secondary|     no|      0|    yes|  no| unknown|     226|       1|   -1|       0|  0|\n",
            "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.format('com.databricks.spark.csv') \\\n",
        "            .options(header='true', inferschema='true') \\\n",
        "            .load(\"/content/drive/MyDrive/bank.csv\",header=True);\n",
        "df.drop('day','month','poutcome').show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "adb1443a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adb1443a",
        "outputId": "fd211b51-d1dd-46f4-8871-e53b76c52692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- y: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f2502c12",
      "metadata": {
        "id": "f2502c12"
      },
      "outputs": [],
      "source": [
        "def get_dummy(df,categoricalCols,continuousCols,labelCol):\n",
        "\n",
        "    from pyspark.ml import Pipeline\n",
        "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "    from pyspark.sql.functions import col\n",
        "\n",
        "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
        "                 for c in categoricalCols ]\n",
        "\n",
        "    # default setting: dropLast=True\n",
        "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
        "                 outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
        "                 for indexer in indexers ]\n",
        "\n",
        "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
        "                                + continuousCols, outputCol=\"features\")\n",
        "\n",
        "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
        "\n",
        "    model=pipeline.fit(df)\n",
        "    data = model.transform(df)\n",
        "\n",
        "    data = data.withColumn('label',col(labelCol))\n",
        "\n",
        "    return data.select('features','label')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dc5b5712",
      "metadata": {
        "id": "dc5b5712"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors # !!!!caution: not from pyspark.mllib.linalg import Vectors\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f53b682",
      "metadata": {
        "id": "2f53b682"
      },
      "source": [
        "### 3. Deal with categorical data and convert the data to dense vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d9922393",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9922393",
        "outputId": "df77145b-40d5-41d8-c85d-6908be794cd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|(29,[8,11,15,16,1...|    0|\n",
            "|(29,[4,11,13,16,1...|    0|\n",
            "|(29,[0,12,14,16,1...|    0|\n",
            "|(29,[0,11,14,16,1...|    0|\n",
            "|(29,[1,11,13,16,1...|    0|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "catcols = ['job','marital','education','default',\n",
        "           'housing','loan','contact','poutcome']\n",
        "\n",
        "num_cols = ['balance', 'duration','campaign','pdays','previous',]\n",
        "labelCol = 'y'\n",
        "\n",
        "data = get_dummy(df,catcols,num_cols,labelCol)\n",
        "data.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "686a80ba",
      "metadata": {
        "id": "686a80ba"
      },
      "source": [
        "### 4. Deal with categorical label and variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c3cac52f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3cac52f",
        "outputId": "3e8483d9-cd1c-4dc4-ad17-1a59a76af832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+------------+\n",
            "|            features|label|indexedLabel|\n",
            "+--------------------+-----+------------+\n",
            "|(29,[8,11,15,16,1...|    0|         0.0|\n",
            "|(29,[4,11,13,16,1...|    0|         0.0|\n",
            "|(29,[0,12,14,16,1...|    0|         0.0|\n",
            "|(29,[0,11,14,16,1...|    0|         0.0|\n",
            "|(29,[1,11,13,16,1...|    0|         0.0|\n",
            "+--------------------+-----+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "# Index labels, adding metadata to the label column\n",
        "labelIndexer = StringIndexer(inputCol='label',\n",
        "                             outputCol='indexedLabel').fit(data)\n",
        "labelIndexer.transform(data).show(5, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a50cb948",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a50cb948",
        "outputId": "ec6e6438-5f12-40a6-ab8d-20d9c7922510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+--------------------+\n",
            "|            features|label|     indexedFeatures|\n",
            "+--------------------+-----+--------------------+\n",
            "|(29,[8,11,15,16,1...|    0|(29,[8,11,15,16,1...|\n",
            "|(29,[4,11,13,16,1...|    0|(29,[4,11,13,16,1...|\n",
            "|(29,[0,12,14,16,1...|    0|(29,[0,12,14,16,1...|\n",
            "|(29,[0,11,14,16,1...|    0|(29,[0,11,14,16,1...|\n",
            "|(29,[1,11,13,16,1...|    0|(29,[1,11,13,16,1...|\n",
            "+--------------------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorIndexer\n",
        "# Automatically identify categorical features, and index them.\n",
        "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer =VectorIndexer(inputCol=\"features\", \\\n",
        "                                  outputCol=\"indexedFeatures\", \\\n",
        "                                  maxCategories=4).fit(data)\n",
        "featureIndexer.transform(data).show(5, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bcf18e",
      "metadata": {
        "id": "f0bcf18e"
      },
      "source": [
        "### 5. Split the data to training and test data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0f059234",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f059234",
        "outputId": "4022981c-8853-4be4-ba9d-c738fd5b4d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------------+-----+\n",
            "|features                                                                                        |label|\n",
            "+------------------------------------------------------------------------------------------------+-----+\n",
            "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-588.0,81.0,4.0,-1.0])|0    |\n",
            "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-105.0,60.0,2.0,-1.0])|0    |\n",
            "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,117.0,635.0,1.0,-1.0])|0    |\n",
            "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,238.0,808.0,1.0,-1.0])|0    |\n",
            "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,407.0,145.0,2.0,-1.0])|0    |\n",
            "+------------------------------------------------------------------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------------------------------------------------------------------------------------+-----+\n",
            "|features                                                                                          |label|\n",
            "+--------------------------------------------------------------------------------------------------+-----+\n",
            "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,11.0,104.0,3.0,-1.0])   |0    |\n",
            "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,466.0,164.0,1.0,-1.0])  |0    |\n",
            "|(29,[0,11,13,16,17,18,19,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,725.0,266.0,1.0,-1.0])  |0    |\n",
            "|(29,[0,11,13,16,17,18,19,21,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,63.0,3.0,-1.0])            |0    |\n",
            "|(29,[0,11,13,16,17,18,20,21,24,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-1746.0,218.0,1.0,-1.0])|0    |\n",
            "+--------------------------------------------------------------------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and test sets (40% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.6, 0.4])\n",
        "\n",
        "trainingData.show(5,False)\n",
        "testData.show(5,False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40edd7d6",
      "metadata": {
        "id": "40edd7d6"
      },
      "source": [
        "### 6. Fit logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b2cfee72",
      "metadata": {
        "id": "b2cfee72"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "logr = LogisticRegression(featuresCol='indexedFeatures', labelCol='indexedLabel')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d4908b",
      "metadata": {
        "id": "93d4908b"
      },
      "source": [
        "### 7. Create Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9bed3151",
      "metadata": {
        "id": "9bed3151"
      },
      "outputs": [],
      "source": [
        "# Convert indexed labels back to original labels.\n",
        "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
        "                               labels=labelIndexer.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b4e981ad",
      "metadata": {
        "id": "b4e981ad"
      },
      "outputs": [],
      "source": [
        "# Chain indexers and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, logr,labelConverter])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8c81adee",
      "metadata": {
        "id": "8c81adee"
      },
      "outputs": [],
      "source": [
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78b11451",
      "metadata": {
        "id": "78b11451"
      },
      "source": [
        "### 8. Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f0d5492f",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0d5492f",
        "outputId": "43ecaa3f-8d0a-4c19-aea4-288db407409a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+--------------+\n",
            "|            features|label|predictedLabel|\n",
            "+--------------------+-----+--------------+\n",
            "|(29,[0,11,13,16,1...|    0|             0|\n",
            "|(29,[0,11,13,16,1...|    0|             0|\n",
            "|(29,[0,11,13,16,1...|    0|             0|\n",
            "|(29,[0,11,13,16,1...|    0|             0|\n",
            "|(29,[0,11,13,16,1...|    0|             0|\n",
            "+--------------------+-----+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "# Select example rows to display.\n",
        "predictions.select(\"features\",\"label\",\"predictedLabel\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3cf2c057",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cf2c057",
        "outputId": "12287a41-c325-440b-bac9-2e62982f6dc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(label=1), Row(label=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "predictions.select('label').distinct().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e139f30",
      "metadata": {
        "id": "1e139f30"
      },
      "source": [
        "### 9. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "90268204",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90268204",
        "outputId": "599dc4a1-d9c6-4eeb-c00b-5da789e2d55d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error = 0.0941112\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "23ac8a83",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ac8a83",
        "outputId": "837bcf5d-b416-4cf5-b686-e304a84ef0ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|                 FPR|                 TPR|\n",
            "+--------------------+--------------------+\n",
            "|                 0.0|                 0.0|\n",
            "|4.187604690117253E-4|0.003164556962025...|\n",
            "|4.187604690117253E-4| 0.00949367088607595|\n",
            "|8.375209380234506E-4|0.012658227848101266|\n",
            "|8.375209380234506E-4|  0.0189873417721519|\n",
            "+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "areaUnderROC: 0.8765981807773043\n"
          ]
        }
      ],
      "source": [
        "lrModel = model.stages[2]\n",
        "trainingSummary = lrModel.summary\n",
        "\n",
        "# Obtain the objective per iteration\n",
        "# objectiveHistory = trainingSummary.objectiveHistory\n",
        "# print(\"objectiveHistory:\")\n",
        "# for objective in objectiveHistory:\n",
        "#     print(objective)\n",
        "\n",
        "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
        "trainingSummary.roc.show(5)\n",
        "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
        "\n",
        "# Set the model threshold to maximize F-Measure\n",
        "fMeasure = trainingSummary.fMeasureByThreshold\n",
        "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head(5)\n",
        "# bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
        "#     .select('threshold').head()['threshold']\n",
        "# lr.setThreshold(bestThreshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "75a3c7d5",
      "metadata": {
        "id": "75a3c7d5"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}